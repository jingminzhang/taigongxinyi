#!/usr/bin/env python3
"""
RSSæ•°æ®è¯»å–æµ‹è¯•å™¨
æµ‹è¯•ä»MongoDBè¯»å–RSSæ–°é—»æ•°æ®ï¼Œå¹¶åˆ†æç´¢å¼•éœ€æ±‚
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional
from src.mcp.swarm_mongodb_client import SwarmMongoDBClient

class RSSDataReader:
    """RSSæ•°æ®è¯»å–å™¨å’Œåˆ†æå™¨"""
    
    def __init__(self, mongodb_client: SwarmMongoDBClient, database_name: str = "news_debate_db"):
        self.mongodb_client = mongodb_client
        self.database_name = database_name
        self.collection_name = "news_articles"
        self.logger = logging.getLogger(__name__)
    
    async def connect_to_database(self) -> bool:
        """è¿æ¥åˆ°æ•°æ®åº“"""
        try:
            result = self.mongodb_client.connect(self.database_name)
            if result.get('success'):
                self.logger.info(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.database_name}")
                return True
            else:
                self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {result}")
                return False
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¼‚å¸¸: {e}")
            return False
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–æ–‡æ¡£æ€»æ•°
            count_result = self.mongodb_client.count_documents(self.collection_name)
            total_count = count_result.get('count', 0) if count_result.get('success') else 0
            
            # è·å–æœ€æ–°çš„å‡ æ¡è®°å½•æ¥åˆ†ææ•°æ®ç»“æ„
            latest_docs = self.mongodb_client.find_documents(
                self.collection_name,
                query={},
                sort={'collected_at': -1},
                limit=5
            )
            
            # è·å–æœ€æ—©çš„è®°å½•
            earliest_docs = self.mongodb_client.find_documents(
                self.collection_name,
                query={},
                sort={'collected_at': 1},
                limit=1
            )
            
            stats = {
                'total_documents': total_count,
                'latest_documents': latest_docs.get('documents', []) if latest_docs.get('success') else [],
                'earliest_document': earliest_docs.get('documents', []) if earliest_docs.get('success') else [],
                'collection_exists': total_count > 0
            }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}
    
    async def analyze_data_structure(self, sample_size: int = 10) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç»“æ„"""
        try:
            # è·å–æ ·æœ¬æ•°æ®
            sample_result = self.mongodb_client.find_documents(
                self.collection_name,
                query={},
                limit=sample_size
            )
            
            if not sample_result.get('success'):
                return {'error': 'æ— æ³•è·å–æ ·æœ¬æ•°æ®'}
            
            documents = sample_result.get('documents', [])
            if not documents:
                return {'error': 'æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£'}
            
            # åˆ†æå­—æ®µç»“æ„
            field_analysis = {}
            for doc in documents:
                for field, value in doc.items():
                    if field not in field_analysis:
                        field_analysis[field] = {
                            'type': type(value).__name__,
                            'sample_values': [],
                            'count': 0
                        }
                    
                    field_analysis[field]['count'] += 1
                    if len(field_analysis[field]['sample_values']) < 3:
                        field_analysis[field]['sample_values'].append(str(value)[:100])  # é™åˆ¶é•¿åº¦
            
            # åˆ†æå¸¸è§æŸ¥è¯¢å­—æ®µ
            query_fields = {
                'title': 'æ ‡é¢˜æœç´¢',
                'category': 'åˆ†ç±»ç­›é€‰',
                'published': 'æ—¶é—´èŒƒå›´æŸ¥è¯¢',
                'collected_at': 'æ”¶é›†æ—¶é—´æ’åº',
                'tags': 'æ ‡ç­¾æœç´¢',
                'source_title': 'æ¥æºç­›é€‰'
            }
            
            return {
                'sample_count': len(documents),
                'field_analysis': field_analysis,
                'recommended_query_fields': query_fields,
                'sample_document': documents[0] if documents else None
            }
            
        except Exception as e:
            self.logger.error(f"æ•°æ®ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    async def test_query_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        performance_results = {}
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢
        test_queries = [
            {
                'name': 'å…¨è¡¨æ‰«æ',
                'query': {},
                'sort': None,
                'limit': 10
            },
            {
                'name': 'æŒ‰æ—¶é—´æ’åº',
                'query': {},
                'sort': {'collected_at': -1},
                'limit': 10
            },
            {
                'name': 'æ ‡é¢˜æ–‡æœ¬æœç´¢',
                'query': {'title': {'$regex': 'å¸‚åœº', '$options': 'i'}},
                'sort': None,
                'limit': 10
            },
            {
                'name': 'åˆ†ç±»ç­›é€‰',
                'query': {'category': 'è´¢ç»æ–°é—»'},
                'sort': None,
                'limit': 10
            },
            {
                'name': 'æ—¶é—´èŒƒå›´æŸ¥è¯¢',
                'query': {
                    'collected_at': {
                        '$gte': datetime.now(timezone.utc) - timedelta(days=7)
                    }
                },
                'sort': {'collected_at': -1},
                'limit': 10
            }
        ]
        
        for test in test_queries:
            try:
                start_time = time.time()
                
                result = self.mongodb_client.find_documents(
                    self.collection_name,
                    query=test['query'],
                    sort=test.get('sort'),
                    limit=test['limit']
                )
                
                end_time = time.time()
                query_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                
                performance_results[test['name']] = {
                    'query_time_ms': round(query_time, 2),
                    'success': result.get('success', False),
                    'document_count': len(result.get('documents', [])),
                    'query': test['query']
                }
                
            except Exception as e:
                performance_results[test['name']] = {
                    'error': str(e),
                    'query': test['query']
                }
        
        return performance_results
    
    async def check_existing_indexes(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç°æœ‰ç´¢å¼•"""
        try:
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨MongoDBçš„åŸç”Ÿå‘½ä»¤æ¥è·å–ç´¢å¼•ä¿¡æ¯
            # ç”±äºSwarmMongoDBClientå¯èƒ½æ²¡æœ‰ç›´æ¥çš„ç´¢å¼•æŸ¥è¯¢æ–¹æ³•ï¼Œæˆ‘ä»¬å°è¯•å…¶ä»–æ–¹å¼
            
            # å°è¯•é€šè¿‡èšåˆç®¡é“è·å–ç´¢å¼•ä¿¡æ¯
            pipeline = [
                {"$indexStats": {}}
            ]
            
            # å¦‚æœå®¢æˆ·ç«¯æ”¯æŒèšåˆæŸ¥è¯¢
            if hasattr(self.mongodb_client, 'aggregate_documents'):
                result = self.mongodb_client.aggregate_documents(
                    self.collection_name,
                    pipeline=pipeline
                )
                
                if result.get('success'):
                    return {
                        'indexes': result.get('documents', []),
                        'method': 'aggregation'
                    }
            
            # å¦‚æœæ— æ³•ç›´æ¥è·å–ç´¢å¼•ä¿¡æ¯ï¼Œè¿”å›å»ºè®®
            return {
                'message': 'æ— æ³•ç›´æ¥æŸ¥è¯¢ç´¢å¼•ä¿¡æ¯ï¼Œå»ºè®®æ‰‹åŠ¨æ£€æŸ¥',
                'method': 'manual_check_needed'
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'message': 'ç´¢å¼•æŸ¥è¯¢å¤±è´¥'
            }
    
    def generate_index_recommendations(self, performance_results: Dict[str, Any], 
                                     data_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç´¢å¼•å»ºè®®"""
        recommendations = {
            'basic_indexes': [],
            'compound_indexes': [],
            'text_indexes': [],
            'vector_indexes': [],
            'reasoning': []
        }
        
        # åŸºç¡€ç´¢å¼•å»ºè®®
        slow_queries = [name for name, result in performance_results.items() 
                       if isinstance(result, dict) and result.get('query_time_ms', 0) > 100]
        
        if slow_queries:
            recommendations['reasoning'].append(f"å‘ç°æ…¢æŸ¥è¯¢: {', '.join(slow_queries)}")
        
        # åŸºäºæ•°æ®ç»“æ„çš„ç´¢å¼•å»ºè®®
        field_analysis = data_analysis.get('field_analysis', {})
        
        # æ—¶é—´å­—æ®µç´¢å¼•ï¼ˆç”¨äºæ’åºå’ŒèŒƒå›´æŸ¥è¯¢ï¼‰
        if 'collected_at' in field_analysis:
            recommendations['basic_indexes'].append({
                'field': 'collected_at',
                'type': 'descending',
                'reason': 'ç”¨äºæ—¶é—´æ’åºå’ŒèŒƒå›´æŸ¥è¯¢'
            })
        
        if 'published' in field_analysis:
            recommendations['basic_indexes'].append({
                'field': 'published',
                'type': 'descending', 
                'reason': 'ç”¨äºå‘å¸ƒæ—¶é—´æŸ¥è¯¢'
            })
        
        # åˆ†ç±»å­—æ®µç´¢å¼•
        if 'category' in field_analysis:
            recommendations['basic_indexes'].append({
                'field': 'category',
                'type': 'ascending',
                'reason': 'ç”¨äºåˆ†ç±»ç­›é€‰'
            })
        
        # å”¯ä¸€æ ‡è¯†ç¬¦ç´¢å¼•
        if 'article_id' in field_analysis:
            recommendations['basic_indexes'].append({
                'field': 'article_id',
                'type': 'ascending',
                'unique': True,
                'reason': 'å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œé˜²æ­¢é‡å¤'
            })
        
        # å¤åˆç´¢å¼•å»ºè®®
        recommendations['compound_indexes'].append({
            'fields': ['category', 'collected_at'],
            'reason': 'æ”¯æŒæŒ‰åˆ†ç±»ç­›é€‰å¹¶æŒ‰æ—¶é—´æ’åº'
        })
        
        # æ–‡æœ¬æœç´¢ç´¢å¼•
        text_fields = []
        for field in ['title', 'description', 'summary']:
            if field in field_analysis:
                text_fields.append(field)
        
        if text_fields:
            recommendations['text_indexes'].append({
                'fields': text_fields,
                'type': 'text',
                'reason': 'æ”¯æŒå…¨æ–‡æœç´¢'
            })
        
        # å‘é‡ç´¢å¼•å»ºè®®
        recommendations['vector_indexes'].append({
            'consideration': 'å¦‚æœéœ€è¦è¯­ä¹‰æœç´¢',
            'fields': ['title', 'description'],
            'method': 'embedding + vector_search',
            'reason': 'ç”¨äºåŸºäºå†…å®¹ç›¸ä¼¼æ€§çš„æ™ºèƒ½æœç´¢å’Œæ¨è'
        })
        
        return recommendations
    
    async def test_sample_queries(self) -> Dict[str, Any]:
        """æµ‹è¯•ä¸€äº›ç¤ºä¾‹æŸ¥è¯¢"""
        sample_queries = {}
        
        try:
            # 1. è·å–æœ€æ–°10æ¡æ–°é—»
            latest_news = self.mongodb_client.find_documents(
                self.collection_name,
                query={},
                sort={'collected_at': -1},
                limit=10
            )
            sample_queries['latest_news'] = {
                'success': latest_news.get('success'),
                'count': len(latest_news.get('documents', [])),
                'sample_titles': [doc.get('title', 'N/A')[:50] + '...' 
                                for doc in latest_news.get('documents', [])[:3]]
            }
            
            # 2. æŒ‰åˆ†ç±»æŸ¥è¯¢
            category_news = self.mongodb_client.find_documents(
                self.collection_name,
                query={'category': 'è´¢ç»æ–°é—»'},
                limit=5
            )
            sample_queries['category_news'] = {
                'success': category_news.get('success'),
                'count': len(category_news.get('documents', [])),
                'category': 'è´¢ç»æ–°é—»'
            }
            
            # 3. å…³é”®è¯æœç´¢
            keyword_search = self.mongodb_client.find_documents(
                self.collection_name,
                query={'title': {'$regex': 'æŠ•èµ„|è‚¡ç¥¨|å¸‚åœº', '$options': 'i'}},
                limit=5
            )
            sample_queries['keyword_search'] = {
                'success': keyword_search.get('success'),
                'count': len(keyword_search.get('documents', [])),
                'keywords': 'æŠ•èµ„|è‚¡ç¥¨|å¸‚åœº'
            }
            
        except Exception as e:
            sample_queries['error'] = str(e)
        
        return sample_queries
    
    async def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åˆ†æ"""
        self.logger.info("å¼€å§‹RSSæ•°æ®åˆ†æ...")
        
        # è¿æ¥æ•°æ®åº“
        if not await self.connect_to_database():
            return {'error': 'æ— æ³•è¿æ¥åˆ°æ•°æ®åº“'}
        
        analysis_results = {}
        
        # 1. è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        self.logger.info("è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯...")
        analysis_results['collection_stats'] = await self.get_collection_stats()
        
        # 2. åˆ†ææ•°æ®ç»“æ„
        self.logger.info("åˆ†ææ•°æ®ç»“æ„...")
        analysis_results['data_structure'] = await self.analyze_data_structure()
        
        # 3. æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        self.logger.info("æµ‹è¯•æŸ¥è¯¢æ€§èƒ½...")
        analysis_results['query_performance'] = await self.test_query_performance()
        
        # 4. æ£€æŸ¥ç°æœ‰ç´¢å¼•
        self.logger.info("æ£€æŸ¥ç°æœ‰ç´¢å¼•...")
        analysis_results['existing_indexes'] = await self.check_existing_indexes()
        
        # 5. ç”Ÿæˆç´¢å¼•å»ºè®®
        self.logger.info("ç”Ÿæˆç´¢å¼•å»ºè®®...")
        analysis_results['index_recommendations'] = self.generate_index_recommendations(
            analysis_results['query_performance'],
            analysis_results['data_structure']
        )
        
        # 6. æµ‹è¯•ç¤ºä¾‹æŸ¥è¯¢
        self.logger.info("æµ‹è¯•ç¤ºä¾‹æŸ¥è¯¢...")
        analysis_results['sample_queries'] = await self.test_sample_queries()
        
        return analysis_results

async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–MongoDBå®¢æˆ·ç«¯
    mongodb_client = SwarmMongoDBClient(
        mcp_server_url="http://localhost:8080",
        default_database="news_debate_db"
    )
    
    # åˆ›å»ºæ•°æ®è¯»å–å™¨
    reader = RSSDataReader(mongodb_client)
    
    # è¿è¡Œåˆ†æ
    results = await reader.run_comprehensive_analysis()
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("RSSæ•°æ®åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # é›†åˆç»Ÿè®¡
    stats = results.get('collection_stats', {})
    print(f"\nğŸ“Š é›†åˆç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {stats.get('total_documents', 0)}")
    print(f"  é›†åˆå­˜åœ¨: {stats.get('collection_exists', False)}")
    
    # æ•°æ®ç»“æ„
    structure = results.get('data_structure', {})
    if 'field_analysis' in structure:
        print(f"\nğŸ—ï¸  æ•°æ®ç»“æ„:")
        for field, info in structure['field_analysis'].items():
            print(f"  {field}: {info['type']} (å‡ºç°{info['count']}æ¬¡)")
    
    # æŸ¥è¯¢æ€§èƒ½
    performance = results.get('query_performance', {})
    print(f"\nâš¡ æŸ¥è¯¢æ€§èƒ½:")
    for query_name, result in performance.items():
        if isinstance(result, dict) and 'query_time_ms' in result:
            print(f"  {query_name}: {result['query_time_ms']}ms ({result['document_count']}æ¡ç»“æœ)")
    
    # ç´¢å¼•å»ºè®®
    recommendations = results.get('index_recommendations', {})
    print(f"\nğŸ’¡ ç´¢å¼•å»ºè®®:")
    
    basic_indexes = recommendations.get('basic_indexes', [])
    if basic_indexes:
        print(f"  åŸºç¡€ç´¢å¼•:")
        for idx in basic_indexes:
            print(f"    - {idx['field']} ({idx.get('type', 'ascending')}): {idx['reason']}")
    
    compound_indexes = recommendations.get('compound_indexes', [])
    if compound_indexes:
        print(f"  å¤åˆç´¢å¼•:")
        for idx in compound_indexes:
            print(f"    - {', '.join(idx['fields'])}: {idx['reason']}")
    
    text_indexes = recommendations.get('text_indexes', [])
    if text_indexes:
        print(f"  æ–‡æœ¬ç´¢å¼•:")
        for idx in text_indexes:
            print(f"    - {', '.join(idx['fields'])}: {idx['reason']}")
    
    vector_indexes = recommendations.get('vector_indexes', [])
    if vector_indexes:
        print(f"  å‘é‡ç´¢å¼•å»ºè®®:")
        for idx in vector_indexes:
            print(f"    - {idx['consideration']}: {idx['reason']}")
    
    # ç¤ºä¾‹æŸ¥è¯¢ç»“æœ
    samples = results.get('sample_queries', {})
    print(f"\nğŸ” ç¤ºä¾‹æŸ¥è¯¢:")
    for query_name, result in samples.items():
        if isinstance(result, dict) and 'count' in result:
            print(f"  {query_name}: {result['count']}æ¡ç»“æœ")
    
    print(f"\n" + "="*60)
    print("åˆ†æå®Œæˆï¼")
    print("="*60)
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    with open('/home/ben/liurenchaxin/rss_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    print("\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: rss_analysis_report.json")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    asyncio.run(main())